# Arquitecturas de Modelos

Este documento explica las diferentes arquitecturas de modelos disponibles en el proyecto y sus caracter√≠sticas t√©cnicas.

## üèõÔ∏è Arquitecturas Disponibles

### 1. EfficientNetB3

**EfficientNet** es una familia de modelos desarrollada por Google que optimiza tanto la precisi√≥n como la eficiencia computacional.

#### Caracter√≠sticas T√©cnicas:
- **Input Shape**: 300x300x3 p√≠xeles
- **Par√°metros**: ~12 millones
- **Depth**: 18 capas principales
- **Width**: Factor de escalamiento 1.2
- **Resolution**: 300x300

#### Ejemplo de Implementaci√≥n:
```python
base_model = EfficientNetB3(
    input_shape=(300, 300, 3),
    include_top=False,        # Sin capa de clasificaci√≥n final
    weights="imagenet"        # Pesos preentrenados en ImageNet
)
base_model.trainable = False  # Congelar capas base para transfer learning
```

#### Ventajas:
- ‚úÖ Excelente balance precisi√≥n/eficiencia
- ‚úÖ Dise√±o escalable y consistente
- ‚úÖ Estado del arte en m√∫ltiples benchmarks

#### Desventajas:
- ‚ùå Mayor uso de memoria que MobileNet
- ‚ùå Tiempo de inferencia m√°s lento en dispositivos m√≥viles

---

### 2. MobileNetV2

**MobileNet** est√° dise√±ado espec√≠ficamente para aplicaciones m√≥viles y dispositivos con recursos limitados.

#### Caracter√≠sticas T√©cnicas:
- **Input Shape**: 224x224x3 p√≠xeles
- **Par√°metros**: ~3.4 millones
- **Arquitectura**: Depthwise Separable Convolutions
- **Inverted Residuals**: Bloques de construcci√≥n principales

#### Ejemplo de Implementaci√≥n:
```python
base_model = MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights="imagenet"
)

# Augmentaci√≥n de datos integrada
augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])
```

#### Ventajas:
- ‚úÖ Muy ligero y r√°pido
- ‚úÖ Ideal para dispositivos m√≥viles
- ‚úÖ Buen rendimiento con pocos recursos

#### Desventajas:
- ‚ùå Menor precisi√≥n que modelos m√°s grandes
- ‚ùå Puede tener dificultades con datos complejos

---

### 3. ResNeSt50

**ResNeSt** (ResNet + Split-Attention Networks) es una mejora de ResNet que incorpora mecanismos de atenci√≥n.

#### Caracter√≠sticas T√©cnicas:
- **Input Shape**: 224x224x3 p√≠xeles
- **Par√°metros**: ~27 millones
- **Bloques**: Split-Attention blocks
- **Profundidad**: 50 capas

#### Ejemplo de Implementaci√≥n:
```python
# Usando TensorFlow Hub
resnet_url = "https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5"
feature_extractor = hub.KerasLayer(
    resnet_url,
    input_shape=(224, 224, 3),
    trainable=False
)

model = tf.keras.Sequential([
    feature_extractor,
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(num_classes, activation='softmax')
])
```

#### Ventajas:
- ‚úÖ Excelente para tareas complejas
- ‚úÖ Mecanismo de atenci√≥n incorporado
- ‚úÖ Robusta con diferentes tipos de datos

#### Desventajas:
- ‚ùå Mayor consumo computacional
- ‚ùå M√°s lenta en inferencia

---

### 4. ConvNeXt-Tiny

**ConvNeXt** es una arquitectura moderna que combina las fortalezas de las redes convolucionales con ideas de los Transformers.

#### Caracter√≠sticas T√©cnicas:
- **Input Shape**: 224x224x3 p√≠xeles
- **Par√°metros**: ~28 millones
- **Arquitectura**: Convoluciones modernizadas con t√©cnicas de Transformers
- **Bloques**: ConvNeXt blocks con LayerScale

#### Ejemplo de Implementaci√≥n:
```python
base_model = ConvNeXtTiny(
    input_shape=(224, 224, 3),
    include_top=False,
    weights="imagenet"
)
base_model.trainable = False

# Capa personalizada LayerScale
class LayerScale(layers.Layer):
    def __init__(self, init_values=1e-6, projection_dim=768, **kwargs):
        super(LayerScale, self).__init__(**kwargs)
        self.init_values = init_values
        self.projection_dim = projection_dim
```

#### Ventajas:
- ‚úÖ Arquitectura de vanguardia (2022)
- ‚úÖ Combina convolutiones con ideas de Transformers
- ‚úÖ Excelente rendimiento en tareas modernas
- ‚úÖ Dise√±o escalable y eficiente

#### Desventajas:
- ‚ùå Relativamente nuevo, menos investigado
- ‚ùå Requiere capas personalizadas (LayerScale)

## üîß Transfer Learning

Todos los modelos implementan **Transfer Learning**:

### ¬øQu√© es Transfer Learning?
Es una t√©cnica donde utilizamos un modelo preentrenado (en ImageNet) y lo adaptamos para nuestra tarea espec√≠fica.

### Proceso:
1. **Congelamos** las capas del modelo base (`trainable = False`)
2. **Agregamos** capas finales espec√≠ficas para nuestro problema
3. **Entrenamos** solo las nuevas capas

### Ejemplo Pr√°ctico:
```python
# Modelo base congelado
base_model.trainable = False

# Nuevas capas entrenables
x = base_model(inputs, training=False)  # No entrenar durante forward pass
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
```

## üìä Comparaci√≥n de Modelos

| Modelo | Tama√±o Input | Par√°metros | Velocidad | Precisi√≥n | Uso RAM |
|--------|-------------|------------|-----------|-----------|---------|
| EfficientNetB3 | 300x300 | ~12M | Medio | Alta | Alto |
| MobileNetV2 | 224x224 | ~3.4M | R√°pido | Media | Bajo |
| ResNeSt50 | 224x224 | ~27M | Lento | Muy Alta | Muy Alto |
| ConvNeXt-Tiny | 224x224 | ~28M | Medio | Muy Alta | Alto |

## üéØ Recomendaciones de Uso

### Para Producci√≥n M√≥vil:
```python
modelo_recomendado = "mobilenet"
```

### Para M√°xima Precisi√≥n:
```python
modelo_recomendado = "resnest"  # o "convnext" para arquitecturas modernas
```

### Para Balance √ìptimo:
```python
modelo_recomendado = "efficientnet"
```

### Para Investigaci√≥n y Experimentaci√≥n:
```python
modelo_recomendado = "convnext"  # √öltima generaci√≥n, arquitectura h√≠brida
```

## üß™ Experimentos y Ablation Studies

### Fine-tuning vs Transfer Learning:
```python
# Transfer Learning (recomendado)
base_model.trainable = False

# Fine-tuning (opcional para datos muy espec√≠ficos)
base_model.trainable = True
# Usar learning rate muy bajo: 1e-5
```

### Comparaci√≥n de Input Sizes:
- **224x224**: Est√°ndar, balance velocidad/precisi√≥n
- **300x300**: Mejor para im√°genes con detalles finos
- **512x512**: M√°xima calidad, pero muy lento

## üìù Consideraciones T√©cnicas

### Memory Management:
```python
# Configurar crecimiento de memoria GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    tf.config.experimental.set_memory_growth(gpus[0], True)
```

### Batch Size Optimization:
```python
# Para GPU con 8GB RAM:
batch_sizes = {
    "mobilenet": 64,      # Ligero
    "efficientnet": 32,   # Medio  
    "resnest": 16,        # Pesado
    "convnext": 32        # Medio-pesado
}
```
